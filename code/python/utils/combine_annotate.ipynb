{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "def preprocess_conll(data):\n",
    "\n",
    "    \"\"\"\n",
    "    Quick preprocessing on the CONLL data so it can be combined\n",
    "    with the Emerging Entities data (which doesn't need any\n",
    "    preprocessing).\n",
    "    \n",
    "    Takes `data`, which is a list of strings read in from\n",
    "    a CONLL data file.\n",
    "    \"\"\"\n",
    "\n",
    "    # Remove DOCSTART lines to make CONLL data consistent\n",
    "    # with the Emerging Entities dataset\n",
    "    data = [line for line in data if 'DOCSTART' not in line]\n",
    "\n",
    "    # Add appropriate tabbing and spacing to match EE data\n",
    "    data = ['\\t'.join([line.split()[0], line.split()[3]]) + '\\n'\n",
    "            if line != '\\n'\n",
    "            else line\n",
    "            for line in data]\n",
    "    return data\n",
    "\n",
    "def create_combined_en_dataset(dataset_path_list, combined_path):\n",
    "\n",
    "    \"\"\"\n",
    "    Takes a dataset_path_list of the two English datasets (can be edited\n",
    "    to accommodate more datasets later), and a combined_path, which\n",
    "    is a path string describing where to save the data.\n",
    "    \n",
    "    Combines the two English datasets such that they have the same formatting;\n",
    "    specifically, each line should look like this: TOKEN\\tLABEL\\n.\n",
    "    See example below.\n",
    "    ['EU\\tB-ORG\\n',\n",
    "    'rejects\\tO\\n',\n",
    "    'German\\tB-MISC\\n',\n",
    "    'call\\tO\\n',\n",
    "    'to\\tO\\n',\n",
    "    'boycott\\tO\\n',\n",
    "    'British\\tB-MISC\\n',\n",
    "    'lamb\\tO\\n',\n",
    "    '.\\tO\\n',\n",
    "    '\\n', ...]\n",
    "    \"\"\"\n",
    "\n",
    "    for path in dataset_path_list:\n",
    "        # indicates that these are the CONLL files\n",
    "        conll_paths = ['test.txt', 'train.txt', 'valid.txt']\n",
    "        if path in ['./data/en/CONLL2003/' + p for p in conll_paths]:\n",
    "            with open(path, 'r') as conll:\n",
    "                conll_data = preprocess_conll(conll.readlines())\n",
    "\n",
    "        else:\n",
    "            with open(path, 'r') as ee:\n",
    "                ee_data = ee.readlines()\n",
    "\n",
    "    # Combine the two datasets\n",
    "    ee_data.extend(conll_data)\n",
    "\n",
    "    # Write out to specified path\n",
    "    with open(combined_path, 'w+') as new:\n",
    "        new.writelines(ee_data)\n",
    "\n",
    "\n",
    "    # Print success message\n",
    "    print('Combined {} and saved new dataset to {}.'\n",
    "         .format(dataset_path_list, combined_path))\n",
    "\n",
    "    return None\n",
    "\n",
    "\n",
    "def map_to_standardized_labels(label):\n",
    "\n",
    "    \"\"\"\n",
    "    Meant to be used w/ pd.apply().\n",
    "    Maps a label to a standardized set of labels, because\n",
    "    the CONLL and EE data include different labelsets and\n",
    "    labeling conventions (EE has a larger # of classes,\n",
    "    and writes out labels as \"person\", \"location\", etc.,\n",
    "    while CONLL uses \"PER\", \"LOC\", and so on).\n",
    "    \"\"\"\n",
    "\n",
    "    if pd.isna(label):\n",
    "        return label\n",
    "\n",
    "    # [:2] keeps the 'B-' or 'I-' part of the label\n",
    "    elif 'loc' in label.lower():\n",
    "        label = label[:2] + 'LOC'\n",
    "\n",
    "    elif 'per' in label.lower():\n",
    "        label = label[:2] + 'PER'\n",
    "\n",
    "    elif any([d in label.lower() for d in ['problem', 'treatment', 'test']]):\n",
    "        label = label[:2] + 'DIS'\n",
    "\n",
    "    elif any([s in label.lower() for s in ['org', 'corp', 'group']]):\n",
    "        label = label[:2] + 'ORG'\n",
    "\n",
    "    # For any leftover labels that are not 'O': map them to MISC\n",
    "    elif label != 'O':\n",
    "        label = label[:2] + 'MISC'\n",
    "\n",
    "    return label\n",
    "\n",
    "\n",
    "def standardize_labels_and_save(dataset_file_list):\n",
    "\n",
    "    \"\"\"\n",
    "    Standardizes the labels for each dataset and saves them\n",
    "    under the same filename for 'standardized'.\n",
    "    \"\"\"\n",
    "\n",
    "    for file in dataset_file_list:\n",
    "\n",
    "        # `sep`, `quoting`, and skip_blank_lines args help preserve data structure\n",
    "        data_df = pd.read_table(\n",
    "                    file, header=None, skip_blank_lines=False,\n",
    "                    sep=' |\\t', quoting=csv.QUOTE_NONE, engine='python'\n",
    "                    ).replace([None], np.nan)\n",
    "\n",
    "        data_df[1] = data_df[1].apply(map_to_standardized_labels)\n",
    "\n",
    "        data_df.to_csv(f'{file[:-4]}.txt', header=False, index=False,\n",
    "                        sep=' ', quoting=csv.QUOTE_NONE)\n",
    "\n",
    "        print(f'Saved standardized data to {file}.')\n",
    "\n",
    "    return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combined ['./data/en/CONLL2003/train.txt', './data/en/emerging_entities_17/wnut17train.conll'] and saved new dataset to ./data/en/train_combined.txt.\n",
      "Combined ['./data/en/CONLL2003/valid.txt', './data/en/emerging_entities_17/emerging.dev.conll'] and saved new dataset to ./data/en/valid_combined.txt.\n",
      "Combined ['./data/en/CONLL2003/test.txt', './data/en/emerging_entities_17/emerging.test.annotated'] and saved new dataset to ./data/en/test_combined.txt.\n",
      "Saved standardized data to ./data/en/train_combined.txt.\n",
      "Saved standardized data to ./data/en/valid_combined.txt.\n",
      "Saved standardized data to ./data/en/test_combined.txt.\n"
     ]
    }
   ],
   "source": [
    "conll_path = './data/en/CONLL2003/'\n",
    "ee_path = './data/en/emerging_entities_17/'\n",
    "\n",
    "combined_path = './data/en/'\n",
    "\n",
    "dataset_filenames = [\"train_combined.txt\", \"valid_combined.txt\", \"test_combined.txt\"]\n",
    "dataset_file_list = [combined_path + fn for fn in dataset_filenames]\n",
    "\n",
    "# Training set\n",
    "create_combined_en_dataset(\n",
    "    [conll_path + \"train.txt\", ee_path + \"wnut17train.conll\"],\n",
    "    combined_path + \"train_combined.txt\",\n",
    ")\n",
    "\n",
    "\n",
    "# Validation set\n",
    "create_combined_en_dataset(\n",
    "    [conll_path + \"valid.txt\", ee_path + \"emerging.dev.conll\"],\n",
    "    combined_path + \"valid_combined.txt\",\n",
    ")\n",
    "\n",
    "# Test set\n",
    "create_combined_en_dataset(\n",
    "    [conll_path + \"test.txt\", ee_path + \"emerging.test.annotated\"],\n",
    "    combined_path + \"test_combined.txt\",\n",
    ")\n",
    "\n",
    "#Standardize the labels on all 3 combined datasets\n",
    "standardize_labels_and_save(dataset_file_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "44934\n",
      "25688\n",
      "70965\n"
     ]
    }
   ],
   "source": [
    "def load_data(file_path):\n",
    "    ''' Converts data from:\n",
    "    word \\t label \\n word \\t label \\n \\n word \\t label\n",
    "    to: sentence, {entities : [(start, end, label), (stard, end, label)]}\n",
    "    '''\n",
    "    file = open(file_path, 'r')\n",
    "    training_data, entities, sentence, unique_labels = [], [], [], []\n",
    "    current_annotation = None\n",
    "    start =0\n",
    "    end = 0 # initialize counter to keep track of start and end characters\n",
    "    for line in file:\n",
    "        line = line.strip(\"\\n\").split(\"\\t\")\n",
    "        # lines with len > 1 are words\n",
    "        if len(line) > 1:\n",
    "            label = line[1]\n",
    "            if(label != 'O'):\n",
    "                label = line[1]+\"_MED\" # the .txt is formatted: label \\t word, label[0:2] = label_type\n",
    "            #label_type = line[0][0] # beginning of annotations - \"B\", intermediate - \"I\"\n",
    "            word = line[0]\n",
    "            sentence.append(word)\n",
    "            start = end\n",
    "            end += (len(word) + 1)  # length of the word + trailing space\n",
    "\n",
    "            if label == 'I_MED' :  # if at the end of an annotation\n",
    "                entities.append(( start,end-1, label))  # append the annotation\n",
    "\n",
    "            if label == 'B_MED': # if beginning new annotation\n",
    "                entities.append(( start,end-1, label))# start annotation at beginning of word\n",
    "\n",
    "\n",
    "\n",
    "            if label != 'O' and label not in unique_labels:\n",
    "                unique_labels.append(label)\n",
    "\n",
    "        # lines with len == 1 are breaks between sentences\n",
    "        if len(line) == 1:\n",
    "            if(len(entities) > 0):\n",
    "                sentence = \" \".join(sentence)\n",
    "                training_data.append([sentence, {'entities' : entities}])\n",
    "            # reset the counters and temporary lists\n",
    "            end = 0 \n",
    "            start = 0\n",
    "            entities, sentence = [], []\n",
    "\n",
    "    file.close()\n",
    "    return training_data, unique_labels\n",
    "\n",
    "\n",
    "\n",
    "train_filenames = ['./data/Disease/BC2GM/train.tsv', './data/Disease/BC4CHEMD/train.tsv', './data/Disease/BC5CDR-chem/train.tsv', './data/Disease/BC5CDR-disease/train.tsv', './data/Disease/JNLPBA/train.tsv', './data/Disease/linnaeus/train.tsv', './data/Disease/NCBI-disease/train.tsv', './data/Disease/s800/train.tsv'] \n",
    "test_filenames = ['./data/Disease/BC2GM/test.tsv', './data/Disease/BC4CHEMD/test.tsv', './data/Disease/BC5CDR-chem/test.tsv', './data/Disease/BC5CDR-disease/test.tsv', './data/Disease/JNLPBA/test.tsv', './data/Disease/linnaeus/test.tsv', './data/Disease/NCBI-disease/test.tsv', './data/Disease/s800/test.tsv'] \n",
    "val_filenames = ['./data/Disease/BC2GM/train_dev.tsv', './data/Disease/BC4CHEMD/train_dev.tsv', './data/Disease/BC5CDR-chem/train_dev.tsv', './data/Disease/BC5CDR-disease/train_dev.tsv', './data/Disease/JNLPBA/train_dev.tsv', './data/Disease/linnaeus/train_dev.tsv', './data/Disease/NCBI-disease/train_dev.tsv', './data/Disease/s800/train_dev.tsv'] \n",
    "# Open file3 in write mode \n",
    "with open('data/Disease/train.txt', 'w') as outfile: \n",
    "  \n",
    "    # Iterate through list \n",
    "    for names in train_filenames: \n",
    "\n",
    "        # Open each file in read mode \n",
    "        with open(names) as infile:\n",
    "            outfile.write(infile.read()) \n",
    "\n",
    "        # Add '\\n' to enter data of file2 \n",
    "        # from next line \n",
    "        outfile.write(\"\\n\")\n",
    "        outfile.write(\"\\n\")\n",
    "    #outfile.close\n",
    "\n",
    "with open('./data/Disease/test.txt', 'w') as outfile: \n",
    "  \n",
    "    # Iterate through list \n",
    "    for names in test_filenames: \n",
    "\n",
    "        # Open each file in read mode \n",
    "        with open(names) as infile:\n",
    "            outfile.write(infile.read()) \n",
    "\n",
    "        # Add '\\n' to enter data of file2 \n",
    "        # from next line \n",
    "        outfile.write(\"\\n\")\n",
    "        outfile.write(\"\\n\")\n",
    "    #outfile.close\n",
    "\n",
    "with open('./data/Disease/valid.txt', 'w') as outfile: \n",
    "  \n",
    "    # Iterate through list \n",
    "    for names in val_filenames: \n",
    "\n",
    "        # Open each file in read mode \n",
    "        with open(names) as infile:\n",
    "            outfile.write(infile.read()) \n",
    "\n",
    "        # Add '\\n' to enter data of file2 \n",
    "        # from next line \n",
    "        outfile.write(\"\\n\")\n",
    "        outfile.write(\"\\n\")\n",
    "    #outfile.close\n",
    "\n",
    "TRAIN_DATA, LABELS = load_data(\"./data/Disease/train.txt\")\n",
    "print(len(TRAIN_DATA))\n",
    "TEST_DATA, _ = load_data(\"./data/Disease/test.txt\")\n",
    "print(len(TEST_DATA))\n",
    "VALID_DATA, _ = load_data(\"./data/Disease/valid.txt\")\n",
    "print(len(VALID_DATA))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# ADD _MED tags\n",
    "file = open('./data/Disease/train.txt', 'r')\n",
    "combined_list = []\n",
    "for line in file:\n",
    "    line = line.strip(\"\\n\").split(\"\\t\")\n",
    "    if len(line) > 1:\n",
    "        label = line[1]\n",
    "        if(label != 'O'):\n",
    "            label = line[1]+\"-MED\"\n",
    "            result_combination = line[0] + ' ' + label\n",
    "            combined_list.append(result_combination)\n",
    "        else:\n",
    "            result_combination = line[0] + ' ' + line[1]\n",
    "            combined_list.append(result_combination)\n",
    "    if len(line) == 1:\n",
    "        combined_list.append(\"\")\n",
    "\n",
    "with open('data/Disease/train.txt', 'w') as outfile:  \n",
    "    for names in combined_list: \n",
    "        outfile.write(names) \n",
    "        outfile.write(\"\\n\")\n",
    "\n",
    "\n",
    "file = open('./data/Disease/test.txt', 'r')\n",
    "combined_list = []\n",
    "for line in file:\n",
    "    line = line.strip(\"\\n\").split(\"\\t\")\n",
    "    if len(line) > 1:\n",
    "        label = line[1]\n",
    "        if(label != 'O'):\n",
    "            label = line[1]+\"-MED\"\n",
    "            result_combination = line[0] + ' ' + label\n",
    "            combined_list.append(result_combination)\n",
    "        else:\n",
    "            result_combination = line[0] + ' ' + line[1]\n",
    "            combined_list.append(result_combination)\n",
    "    if len(line) == 1:\n",
    "        combined_list.append(\"\")\n",
    "\n",
    "with open('./data/Disease/test.txt', 'w') as outfile:  \n",
    "    for names in combined_list: \n",
    "        outfile.write(names) \n",
    "        outfile.write(\"\\n\")\n",
    "\n",
    "\n",
    "file = open('./data/Disease/valid.txt', 'r')\n",
    "combined_list = []\n",
    "for line in file:\n",
    "    line = line.strip(\"\\n\").split(\"\\t\")\n",
    "    if len(line) > 1:\n",
    "        label = line[1]\n",
    "        if(label != 'O'):\n",
    "            label = line[1]+\"-MED\"\n",
    "            result_combination = line[0] + ' ' + label\n",
    "            combined_list.append(result_combination)\n",
    "        else:\n",
    "            result_combination = line[0] + ' ' + line[1]\n",
    "            combined_list.append(result_combination)\n",
    "    if len(line) == 1:\n",
    "        combined_list.append(\"\")\n",
    "\n",
    "with open('./data/Disease/valid.txt', 'w') as outfile:  \n",
    "    for names in combined_list: \n",
    "        outfile.write(names) \n",
    "        outfile.write(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Med_Bank created\n",
      "train processed!\n",
      "valid processed!\n",
      "test processed!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "dis_list = []\n",
    "dis_dict = {}\n",
    "Med_Bank = [\"./data/Disease/train.txt\", \"./data/Disease/valid.txt\", \"./data/Disease/test.txt\"]\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "for filenames in Med_Bank:\n",
    "    with open(filenames) as f_in:\n",
    "        lines = (line.rstrip() for line in f_in)\n",
    "        SF = (line for line in lines if line)\n",
    "        for line in SF:\n",
    "            a = line.split()\n",
    "            if a[1] != \"O\" and a[0].isalpha() and a[0].lower() not in stop_words:\n",
    "                dis_list.append(a[0])\n",
    "                dis_dict[a[0]] = a[1]\n",
    "print(\"Med_Bank created!\")\n",
    "samples = dis_list\n",
    "\n",
    "\n",
    "b=\"\"\n",
    "fin = open(\"./data/en/train_combined.txt\", \"rt\")\n",
    "fout = open(\"./data/train_en.txt\", \"wt\")\n",
    "for line in fin:\n",
    "    if line.strip() == '' or line.strip() == '\\n' or line.strip() == 'O' or line.lower() == 'o':\n",
    "        b += \"\\n\"\n",
    "    else:\n",
    "        a = line.split()\n",
    "        if a[0] in samples:\n",
    "            a[1] = dis_dict[a[0]]\n",
    "        b += \"\"+a[0]+\" \"+a[1] + \"\\n\"\n",
    "fout.write(b)\n",
    "fin.close()\n",
    "fout.close()\n",
    "print(\"train processed!\")\n",
    "\n",
    "\n",
    "b=\"\"\n",
    "fin = open(\"./data/en/valid_combined.txt\", \"rt\")\n",
    "fout = open(\"./data/valid_en.txt\", \"wt\")\n",
    "for line in fin:\n",
    "    if line.strip() == '' or line.strip() == '\\n' or line.strip() == 'O' or line.lower() == 'o':\n",
    "        b += \"\\n\"\n",
    "    else:\n",
    "        a = line.split()\n",
    "        if a[0] in samples:\n",
    "            a[1] = dis_dict[a[0]]\n",
    "        b += \"\"+a[0]+\" \"+a[1] + \"\\n\"\n",
    "fout.write(b)\n",
    "fin.close()\n",
    "fout.close()\n",
    "print(\"valid processed!\")\n",
    "\n",
    "\n",
    "b=\"\"\n",
    "fin = open(\"./data/en/test_combined.txt\", \"rt\")\n",
    "fout = open(\"./data/test_en.txt\", \"wt\")\n",
    "for line in fin:\n",
    "    if line.strip() == '' or line.strip() == '\\n' or line.strip() == 'O' or line.lower() == 'o':\n",
    "        b += \"\\n\"\n",
    "    else:\n",
    "        a = line.split()\n",
    "        if a[0] in samples:\n",
    "            a[1] = dis_dict[a[0]]\n",
    "        b += \"\"+a[0]+\" \"+a[1] + \"\\n\"\n",
    "fout.write(b)\n",
    "fin.close()\n",
    "fout.close()\n",
    "print(\"test processed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "En_Bank created!\n",
      "train processed!\n",
      "valid processed!\n",
      "test processed!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "en_list = []\n",
    "en_dict = {}\n",
    "En_Bank = [\"./data/en/train_combined.txt\", \"./data/en/valid_combined.txt\", \"./data/en/test_combined.txt\"]\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "for filenames in En_Bank:\n",
    "    with open(filenames) as f_in:\n",
    "        lines = (line.rstrip() for line in f_in)\n",
    "        SF = (line for line in lines if line)\n",
    "        for line in SF:\n",
    "            if line.strip() != '' and line.strip() != '\\n' and line.strip() != 'O' and line.lower() != 'o':\n",
    "#                 print(line)\n",
    "                a = line.split()\n",
    "#                 print(a)\n",
    "                if a[1] != \"O\" and a[0].isalpha() and a[0].lower() not in stop_words:\n",
    "                    en_list.append(a[0])\n",
    "                    en_dict[a[0]] = a[1]\n",
    "print(\"En_Bank created!\")\n",
    "samples = en_list\n",
    "\n",
    "\n",
    "b=\"\"\n",
    "fin = open(\"./data/Disease/train.txt\", \"rt\")\n",
    "fout = open(\"./data/train_med.txt\", \"wt\")\n",
    "for line in fin:\n",
    "    if line.strip() == '' or line.strip() == '\\n' or line.strip() == 'O' or line.lower() == 'o':\n",
    "        b += \"\\n\"\n",
    "    else:\n",
    "        a = line.split()\n",
    "        if a[0] in samples:\n",
    "            a[1] = en_dict[a[0]]\n",
    "        b += \"\"+a[0]+\" \"+a[1] + \"\\n\"\n",
    "fout.write(b)\n",
    "fin.close()\n",
    "fout.close()\n",
    "print(\"train processed!\")\n",
    "\n",
    "\n",
    "b=\"\"\n",
    "fin = open(\"./data/Disease/valid.txt\", \"rt\")\n",
    "fout = open(\"./data/valid_med.txt\", \"wt\")\n",
    "for line in fin:\n",
    "    if line.strip() == '' or line.strip() == '\\n' or line.strip() == 'O' or line.lower() == 'o':\n",
    "        b += \"\\n\"\n",
    "    else:\n",
    "        a = line.split()\n",
    "        if a[0] in samples:\n",
    "            a[1] = en_dict[a[0]]\n",
    "        b += \"\"+a[0]+\" \"+a[1] + \"\\n\"\n",
    "fout.write(b)\n",
    "fin.close()\n",
    "fout.close()\n",
    "print(\"valid processed!\")\n",
    "\n",
    "\n",
    "b=\"\"\n",
    "fin = open(\"./data/Disease/test.txt\", \"rt\")\n",
    "fout = open(\"./data/test_med.txt\", \"wt\")\n",
    "for line in fin:\n",
    "    if line.strip() == '' or line.strip() == '\\n' or line.strip() == 'O' or line.lower() == 'o':\n",
    "        b += \"\\n\"\n",
    "    else:\n",
    "        a = line.split()\n",
    "        if a[0] in samples:\n",
    "            a[1] = en_dict[a[0]]\n",
    "        b += \"\"+a[0]+\" \"+a[1] + \"\\n\"\n",
    "fout.write(b)\n",
    "fin.close()\n",
    "fout.close()\n",
    "print(\"test processed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_med_dir = './data/train_med.txt'\n",
    "valid_med_dir = './data/valid_med.txt'\n",
    "test_med_dir = './data/test_med.txt'\n",
    "with open(train_med_dir, 'r') as train_med:\n",
    "    train_med_data = train_med.readlines()\n",
    "with open(valid_med_dir, 'r') as valid_med:\n",
    "    valid_med_data = valid_med.readlines()\n",
    "with open(test_med_dir, 'r') as test_med:\n",
    "    test_med_data = test_med.readlines()\n",
    "\n",
    "    \n",
    "train_en_dir = './data/train_en.txt'\n",
    "valid_en_dir = './data/valid_en.txt'\n",
    "test_en_dir = './data/test_en.txt'\n",
    "with open(train_en_dir, 'r') as train_en:\n",
    "    train_en_data = train_en.readlines()\n",
    "with open(valid_en_dir, 'r') as valid_en:\n",
    "    valid_en_data = valid_en.readlines()\n",
    "with open(test_en_dir, 'r') as test_en:\n",
    "    test_en_data = test_en.readlines()    \n",
    "\n",
    "\n",
    "train_en_data.extend(train_med_data)\n",
    "valid_en_data.extend(valid_med_data)\n",
    "test_en_data.extend(test_med_data)\n",
    "\n",
    "\n",
    "with open('./data/train.txt', 'w+') as trains:\n",
    "    trains.writelines(train_en_data)\n",
    "with open('./data/valid.txt', 'w+') as valids:\n",
    "    valids.writelines(valid_en_data)\n",
    "with open('./data/test.txt', 'w+') as tests:\n",
    "    tests.writelines(test_en_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
